{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the seminar! We hope you will find it useful and fun.\n",
    "Before we start, let's make sure you have everything ready.\n",
    "\n",
    "## Installing python\n",
    "\n",
    "If you are reading this inside a jupyter notebook, likely it's already installed.\n",
    "Besides python, you will need `jupyter`, `numpy` and `julia` libraries, you can install them using `pip install`.\n",
    "We have tested this notebook under python3, but python2 should work too.\n",
    "\n",
    "## Installing julia\n",
    "\n",
    "You will also need to install the julia interpreter. Go to https://julialang.org/downloads/ and get yourself a v1.1+ that suits your operating system.\n",
    "If you download julia as a binary, unpack it somewhere and add this to your `.bashrc`:\n",
    "```\n",
    "export PATH=\"/path/to/julia/bin:$PATH\n",
    "```\n",
    "You might need to reload kernel after you update `PATH`.\n",
    "\n",
    "Type `julia` in the terminal, if you see a nice ascii art with a julia logo and the interpreter prompt it worked well. Now you need to install the AdaGram package (lives here: https://bitbucket.org/sbos/adagram_deepbayes2019). In the julia interpreter type in the following command:\n",
    "```\n",
    "using Pkg\n",
    "Pkg.add(PackageSpec(url=\"https://bitbucket.org/sbos/adagram_deepbayes2019.git\"))\n",
    "```\n",
    "\n",
    "## Download an AdaGram model\n",
    "\n",
    "If you have wget installed, you can simply run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://w2v.s3.amazonaws.com/huang_super_200D_0.1_min20_hs_t1e-17.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, download this file manually to the same dir where this notebook is located (or remember the path and modify it below accordingly)\n",
    "\n",
    "Also, `git clone https://bitbucket.org/sbos/adagram_deepbayes2019.git` somewhere on your laptop. Further we will refer to this directory as `./adagram_deepbayes2019`.\n",
    "\n",
    "## Check that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from julia.api import Julia\n",
    "# if this cell fails, uncomment the line below\n",
    "# jl = Julia(compiled_modules=False)\n",
    "import julia\n",
    "julia.install()\n",
    "from julia import AdaGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm, vocab = AdaGram.load_model(\"huang_super_200D_0.1_min20_hs_t1e-17.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior probabilities of senses of the word \"apple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.07910968e-01, 2.38101072e-01, 2.53984509e-01, 3.13751697e-06,\n",
       "       2.85204898e-07, 2.59277179e-08, 2.35706526e-09, 2.14278660e-10,\n",
       "       1.94798782e-11, 1.77089802e-12, 1.60990729e-13, 1.46355208e-14,\n",
       "       1.33050189e-15, 1.20954717e-16, 1.09958834e-17, 9.99625763e-19,\n",
       "       9.08750694e-20, 8.26136994e-21, 7.51033631e-22, 6.82757847e-23,\n",
       "       6.20688951e-24, 5.64262683e-25, 5.12966076e-26, 4.66332796e-27,\n",
       "       4.23938905e-28, 3.85399005e-29, 3.50362732e-30, 3.18511574e-31,\n",
       "       2.89555977e-32, 2.89555977e-33])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaGram.expected_pi(vm, vocab.word2id[\"apple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours of the first sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macintosh', 1, 0.823969841003418),\n",
       " ('computers', 1, 0.7634434103965759),\n",
       " ('ibm', 1, 0.7630185484886169),\n",
       " ('intel-based', 1, 0.7424857020378113),\n",
       " ('iigs', 1, 0.7415772676467896),\n",
       " ('pc', 1, 0.7379290461540222),\n",
       " ('ms-dos', 1, 0.7352752685546875),\n",
       " ('kaypro', 1, 0.7326763272285461),\n",
       " ('powerpc-based', 1, 0.7302876114845276),\n",
       " ('dos', 1, 0.7282707095146179)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaGram.nearest_neighbors(vm, vocab, \"apple\", 1, min_count=2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours of the first sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pomegranate', 1, 0.8346080183982849),\n",
       " ('almond', 1, 0.8157211542129517),\n",
       " ('apricot', 1, 0.8051078915596008),\n",
       " ('plum', 1, 0.7945712804794312),\n",
       " ('peach', 1, 0.7862921357154846),\n",
       " ('cherry', 1, 0.7756718993186951),\n",
       " ('tamarind', 1, 0.7648524641990662),\n",
       " ('pear', 1, 0.7564710378646851),\n",
       " ('lemon', 1, 0.7523407936096191),\n",
       " ('blueberry', 1, 0.7521582245826721)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaGram.nearest_neighbors(vm, vocab, \"apple\", 2, min_count=2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.56448855e-05, 9.81135272e-01, 1.88190832e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaGram.disambiguate(vm, vocab, \"apple\", \"fresh tasty breakfast\".split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that worked well, we are ready to start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## How many clusters one should expect in a Dirichlet process mixture model?\n",
    "\n",
    "Remember from the lecture, that a measure $G \\sim DP(\\alpha, H)$ has a countable number of distinct values of $\\theta \\sim G$. This allows us to use Dirichlet processes as priors over assignments, e.g. cluster assingments in a mixture model.\n",
    "\n",
    "Formally, if $\\theta_1, \\ldots, \\theta_n \\sim G$ one can write the following predictive distribution over $\\theta_{n+1}$:\n",
    "\n",
    "$$\n",
    "    \\theta_{n+1} | \\theta_{1}, \\ldots, \\theta_{n} \\sim \\frac{1}{\\alpha + n} ( \\alpha H + \\sum_{i=1}^n \\delta(\\theta - \\theta_i)).\n",
    "$$\n",
    "\n",
    "From this formula, it is clear that some $\\theta$s will be equal. Defining $K$ as the number of distinct $\\theta$s and $n_k$ as the number of $\\theta$s equal to the $k$-th value $\\theta_k$, we can rewrite this equation:\n",
    "\n",
    "$$\n",
    "    \\theta_{n+1} | \\theta_{1}, \\ldots, \\theta_{n} \\sim \\frac{1}{\\alpha + n} ( \\alpha H + \\sum_{k=1}^K n_k \\delta(\\theta - \\theta_k)).\n",
    "$$\n",
    "\n",
    "This gives rise to the famous Chinese Restaurant Process, where each $\\theta_k$ is a parameter associated with the $k$-th table. The $n+1$-th customer is choosing between the existing $K$ tables, each of which can be chooses with probability $\\propto n_k$ and creating a new one with probability $\\propto \\alpha$.\n",
    "If the $n+1$-th customer chooses $\\theta_k$, then $n_k$ increases  by 1, otherwise $\\theta_{K+1}$ gets sampled from the base measure $H$ and we set $n_{K+1} = 1$. This procedure then repeats.\n",
    "\n",
    "Now, when we know this, let's try to think how many clusters one should get asymptotically as $n \\rightarrow \\infty$ in expectation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: $\\mathbb{E} K = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational approximation for the truncated stick-breaking process\n",
    "\n",
    "Consider the standard stick-breaking construction of the Dirichlet process.\n",
    "1. Stick-breaking proportions are sampled. $\\beta_k \\sim \\text{Beta}(1, \\alpha), \\quad k=1,\\ldots,\\infty$.\n",
    "2. Parameters are sampled. $\\theta_k \\sim H, \\quad k=1,\\ldots,\\infty$. \n",
    "3. Objects are assigned to clusters according to the stick lengths $\\pi_k$. $p(z_i = k) = \\pi_k, \\quad \\pi_k = \\beta_k \\prod_{t=1}^{k-1} (1 - \\beta_t)$\n",
    "4. $x_i | z_i \\sim p(\\cdot | \\theta_{z_i})$\n",
    "\n",
    "After we observe data $\\mathbf{x} = \\{ x_1, x_2, \\ldots, x_n \\}$ we want to infer the posterior over DP parameters, in this case, $\\mathbf{\\beta}$ and $\\boldsymbol{\\theta}$ as well as the cluster assignments $\\mathbf{z}$.\n",
    "As it is often the case at this summer school, we wish to do that using variational inference. \n",
    "\n",
    "We choose a **finite**, fully-factorized family of variational approximations:\n",
    "$$\n",
    "    q(\\mathbf{\\beta}, \\mathbf{z}, \\boldsymbol{\\theta}) = \\prod_{k=1}^K \\left[ q(\\beta_k) q(\\theta_k) \\right] \\prod_{i=1}^n q(z_i) \\approx p(\\mathbf{\\beta}, \\mathbf{z}, \\boldsymbol{\\theta} | \\mathbf{x}).\n",
    "$$\n",
    "In this approximation, only $K$ clusters are modelled and we choose $q(\\beta_K = 1) = 1$ which automatically assigns zero probability mass to all succeeding clusters.\n",
    "\n",
    "Assume you are given with $q(z_i)$ for each object $x_i$. Derive a variational update for $q^*(\\boldsymbol{\\beta}) = \\arg\\min_{q(\\boldsymbol{\\theta})} \\text{KL}( q(\\mathbf{\\beta}, \\mathbf{z}, \\boldsymbol{\\theta}) || p(\\mathbf{\\beta}, \\mathbf{z}, \\boldsymbol{\\theta} | \\mathbf{x}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: $q^*(\\beta_k) = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus question**: look at the resulting parametrization. Does it look anyhow redundant? Could you use less parameters to fully describe $q^*(\\boldsymbol{\\beta})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "Remember you have just computed the expected number of clusters in a DP?\n",
    "Let's see if the AdaGram model follows this analysis.\n",
    "\n",
    "Plot the number of senses found for each word (the function `AdaGram.expected_pi` used a few cells above is helpful). Choose a reasonable treshold probability to prune out unused samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  448927\n",
      "28895\n"
     ]
    }
   ],
   "source": [
    "# The set of all words known to the model\n",
    "print(\"Total words: \", len(vocab.word2id.keys()))\n",
    "\n",
    "# Word frequencies\n",
    "print(vm.frequencies[vocab.word2id[\"apple\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train some models! Unfortunately, many interesting properties of AdaGram can only be assessed when training on a relatively large corpus. It may be complicated during the time and compute limited practical session, so instead we will train models on synthetic data.\n",
    "\n",
    "In our example, the word **a** can be encountered in two different contexts, one described by words **b** and **c** and the other one by **f** and **g**. So a good model should be able to discover these two \"senses\" of **a**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating train data\n",
    "for _ in range(100):\n",
    "    !echo \"b c b c b c a b c b c b b g f g a g g f g a a g f g\" >> synth_train.txt\n",
    "\n",
    "# generating test data\n",
    "for _ in range(100):\n",
    "    !echo \"a c b b b a b c c f g g a g f g g a b c a b b c a c b f g f g a g f a g\" >> synth_test.txt   \n",
    "    \n",
    "# you are more than welcome to generate something more interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'adagram_deepbayes2019'...\n",
      "remote: Counting objects: 509, done.\u001b[K\n",
      "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
      "remote: Total 509 (delta 333), reused 509 (delta 333)\u001b[KB/s     \n",
      "Receiving objects: 100% (509/509), 10.13 MiB | 148.00 KiB/s, done.\n",
      "Resolving deltas: 100% (333/333), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://bitbucket.org/sbos/adagram_deepbayes2019.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now prepare a dictionary file, this is needed only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./adagram_deepbayes2019/utils/dictionary.sh ./synth_train.txt ./synth.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary... Done!\n",
      "      From worker 2:\t64000 words read, 3412/5200\n",
      "      From worker 2:\t3.85% -1.3335 0.0240 0.0240 2.98/3.00 24.45 kwords/sec\n",
      "      From worker 2:\t7.69% -1.3210 0.0231 0.0231 2.99/3.00 40.15 kwords/sec\n",
      "      From worker 2:\t11.54% -1.3065 0.0221 0.0221 2.99/3.00 38.05 kwords/sec\n",
      "      From worker 2:\t15.38% -1.2951 0.0212 0.0212 3.00/3.00 39.41 kwords/sec\n",
      "      From worker 2:\t19.23% -1.2881 0.0202 0.0202 3.00/3.00 39.07 kwords/sec\n",
      "      From worker 2:\t23.08% -1.2833 0.0192 0.0192 3.00/3.00 38.95 kwords/sec\n",
      "      From worker 2:\t64000 words read, 1624/5200\n",
      "      From worker 2:\t28.46% -1.2785 0.0179 0.0179 3.00/3.00 39.66 kwords/sec\n",
      "      From worker 2:\t32.31% -1.2758 0.0169 0.0169 3.00/3.00 37.32 kwords/sec\n",
      "      From worker 2:\t36.15% -1.2737 0.0160 0.0160 3.00/3.00 36.16 kwords/sec\n",
      "      From worker 2:\t40.00% -1.2718 0.0150 0.0150 3.00/3.00 37.71 kwords/sec\n",
      "      From worker 2:\t43.85% -1.2702 0.0140 0.0140 3.00/3.00 37.34 kwords/sec\n",
      "      From worker 2:\t47.69% -1.2687 0.0131 0.0131 3.00/3.00 38.01 kwords/sec\n",
      "      From worker 2:\t64000 words read, 5034/5200\n",
      "      From worker 2:\t53.08% -1.2669 0.0117 0.0117 3.00/3.00 40.44 kwords/sec\n",
      "      From worker 2:\t56.92% -1.2657 0.0108 0.0108 3.00/3.00 39.97 kwords/sec\n",
      "      From worker 2:\t60.77% -1.2645 0.0098 0.0098 3.00/3.00 41.05 kwords/sec\n",
      "      From worker 2:\t64.62% -1.2634 0.0088 0.0088 3.00/3.00 40.50 kwords/sec\n",
      "      From worker 2:\t68.46% -1.2624 0.0079 0.0079 3.00/3.00 37.86 kwords/sec\n",
      "      From worker 2:\t72.31% -1.2614 0.0069 0.0069 3.00/3.00 38.10 kwords/sec\n",
      "      From worker 2:\t64000 words read, 3246/5200\n",
      "      From worker 2:\t77.69% -1.2601 0.0056 0.0056 3.00/3.00 37.10 kwords/sec\n",
      "      From worker 2:\t81.54% -1.2592 0.0046 0.0046 3.00/3.00 38.69 kwords/sec\n",
      "      From worker 2:\t85.38% -1.2583 0.0037 0.0037 3.00/3.00 38.74 kwords/sec\n",
      "      From worker 2:\t89.23% -1.2575 0.0027 0.0027 3.00/3.00 39.29 kwords/sec\n",
      "      From worker 2:\t93.08% -1.2566 0.0017 0.0017 3.00/3.00 38.24 kwords/sec\n",
      "      From worker 2:\t96.92% -1.2558 0.0008 0.0008 3.00/3.00 38.65 kwords/sec\n",
      "      From worker 2:\t64000 words read, 1454/5200\n",
      "Learning complete 260001 / 260000.0\n"
     ]
    }
   ],
   "source": [
    "!julia ./adagram_deepbayes2019/train.jl --window 3 --min-freq 1 --prototypes 3 \\\n",
    "       --alpha 1. --epochs 100 synth_train.txt synth.vocab synth_prot3_alpha1.model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the parameters `--prototypes` and `--alpha`. Here we used allowed the DP to have up to 3 mixture component for each word and set $\\alpha = 1$.\n",
    "\n",
    "Now we can assess the test likelihood of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\t3400 words read, 7200/7200\n",
      "      From worker 2:\t0 words read, 7200/7200\n",
      "-1.6320696059620439\n",
      "-1.6320696059620439\n"
     ]
    }
   ],
   "source": [
    "!julia ./adagram_deepbayes2019/likelihood.jl --window 3 synth_prot3_alpha1.model synth_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last number printed is the likelihood we are looking for. \n",
    "\n",
    "Now play with the parameters and see if you can extract two different senses of the word **a**. \n",
    "Is this model better than the standard skip-gram (`--prototypes 1`) in terms of the test likelihood?\n",
    "If you forgot how to use AdaGram, see examples in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
